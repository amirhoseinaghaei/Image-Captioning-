{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsKAYIhxq9N6kHK84MZpxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirhoseinaghaei/Image-Captioning-/blob/main/Image_Captioning_For_Flicker_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "l-2yEzTrr8Da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOq2kCor96Vg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models , Sequential\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm \n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, Conv2D , MaxPooling2D   , Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting dataset from google drive"
      ],
      "metadata": {
        "id": "jBFDvuzKsCFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')\n",
        "BASE_DIR = \"gdrive/MyDrive/Flicker_Dataset\"\n",
        "WORKING_DIR  = \"gdrive/MyDrive/Image_Captioning_Project \""
      ],
      "metadata": {
        "id": "Y5_970VnQSPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrained VGG16 model "
      ],
      "metadata": {
        "id": "9Y8g9n2UNJlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model \n",
        "model = VGG16()\n",
        "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AlmT_hKwDfDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Flicker dataset images and extract the features with VGG16 model "
      ],
      "metadata": {
        "id": "t2fdXDUCNfTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the flicker-dataset images and extracting featueres\n",
        "from tqdm import tqdm\n",
        "features = {}\n",
        "img_list = []\n",
        "directory = os.path.join(BASE_DIR, \"Images\")\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "  image_path = directory  + \"/\" + img_name\n",
        "  image = load_img(image_path , target_size= (224,224))\n",
        "  image = img_to_array(image) \n",
        "  image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
        "  feature = model.predict(image)\n",
        "  # img_list.append(image)\n",
        "  image_id = img_name.split(\".\")[0]\n",
        "  features[image_id]  = feature\n"
      ],
      "metadata": {
        "id": "htkZiGPqGKe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the extracted features "
      ],
      "metadata": {
        "id": "t4sp5eQFNdg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving features dictionary in pkl file \n",
        "import pickle\n",
        "pickle.dump(features , open(os.path.join(WORKING_DIR , \"features.pkl\"), \"wb\"))"
      ],
      "metadata": {
        "id": "85k5VMHV2ono"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Captions for each image"
      ],
      "metadata": {
        "id": "S8g_J__KXEaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(BASE_DIR, \"captions.txt\"), 'r') as f: \n",
        "  next(f)\n",
        "  captions = f.read()"
      ],
      "metadata": {
        "id": "ygenXGv5T5qc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {}\n",
        "line = captions.split(\"\\n\")\n",
        "for i in line:\n",
        "  splitted = i.split(\",\")\n",
        "  if len(line) < 2:\n",
        "    continue\n",
        "  img_id = splitted[0].split(\".\")[0]\n",
        "  # print(splitted[1])\n",
        "  caption = splitted[1:]\n",
        "  caption = \" \".join(caption)\n",
        "  if img_id not in mapping.keys():\n",
        "     mapping[img_id] = []\n",
        "  # print(image_id + \": \" + caption)\n",
        "  mapping[img_id].append(caption)\n"
      ],
      "metadata": {
        "id": "Ic_Ot9JMYJhm"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model"
      ],
      "metadata": {
        "id": "JKjIbfuOq-AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.layers.merge import Add\n",
        "from tensorflow.python.ops.gen_array_ops import InplaceAdd\n",
        "from keras.backend import conv2d\n",
        "\n",
        "from keras import optimizers\n",
        "from tensorflow.python.ops.nn_ops import relu\n",
        "def Build_CNN_FeatureExtractor(vocab_size):\n",
        "      input1 = Input(shape=(224,224,3))\n",
        "      conv1 = Conv2D(64, (3, 3), activation= tf.nn.relu, padding=\"same\")(input1)\n",
        "      conv2 = Conv2D(64, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv1)\n",
        "      max1 =  MaxPooling2D((2,2), (2,2))(conv2)\n",
        "      conv3 = Conv2D(128, (3, 3), activation= tf.nn.relu, padding=\"same\")(max1)\n",
        "      conv4 = Conv2D(128, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv3)\n",
        "      max2 =  MaxPooling2D((2,2), (2,2))(conv4)\n",
        "      conv5 = Conv2D(256, (3, 3), activation= tf.nn.relu, padding=\"same\")(max2)\n",
        "      conv6 = Conv2D(256, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv5)\n",
        "      conv7 = Conv2D(256, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv6)\n",
        "      max3 =  MaxPooling2D((2,2), (2,2))(conv7)\n",
        "      conv8 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(max3)\n",
        "      conv9 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv8)\n",
        "      conv10 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv9)\n",
        "      max4 =  MaxPooling2D((2,2), (2,2))(conv10)\n",
        "      conv11 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(max4)\n",
        "      conv12 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv11)\n",
        "      conv13 = Conv2D(512, (3, 3), activation= tf.nn.relu, padding=\"same\")(conv12)\n",
        "      max5 =  MaxPooling2D((2,2), (2,2))(conv13)\n",
        "      flatten = Flatten()(max5)\n",
        "      dense1 = Dense(4096, activation = tf.nn.relu)(flatten)\n",
        "      dropout1 = Dropout(0.4)(dense1)\n",
        "      dense2 = Dense(4096)(dropout1)\n",
        "      dense3 = Dense(256)(dense2)\n",
        "      input2 = Input(shape = (35,))\n",
        "      embedding = Embedding(input_dim = vocab_size, output_dim = 256)(input2)\n",
        "      dropout2 = Dropout(0.4)(embedding)\n",
        "      lstm = LSTM(256)(dropout2)\n",
        "      added = add([dense3, lstm])\n",
        "      dense4 = Dense(256 , activation = tf.nn.relu)(added)\n",
        "      output = Dense(vocab_size, activation = tf.nn.softmax)(dense4)\n",
        "      model = Model(inputs = [input1 , input2], outputs = output)\n",
        "      return model\n",
        " \n",
        "vocab_size = 8600\n",
        "model = Build_CNN_FeatureExtractor(vocab_size)\n",
        "optimizer = tf.optimizers.Adam()\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\" , metrics = [\"Accuracy\"] )\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YD7OsfvsaD10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the model"
      ],
      "metadata": {
        "id": "-DCeXTnhsIbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model= model)"
      ],
      "metadata": {
        "id": "fiBGmQrTq3df"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}